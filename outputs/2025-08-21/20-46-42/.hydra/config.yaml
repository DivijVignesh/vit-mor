name: imagenet_vit_mor_optimized
wandb: true
wandb_mode: online
wandb_entity: divijvignesh1223
wandb_project: vit-mor-experiments
wandb_run_name: null
output_dir: vit_mor_imagenet
wandb_run_id: Fh03A9ui
tensorboard: false
tensorboard_dir: null
resume_from_checkpoint: false
resume_step: null
model: vit_mor
dataset: imagenet1k
image_size: 224
patch_size: 16
num_labels: 1000
hidden_size: 768
num_hidden_layers: 7
num_attention_heads: 7
intermediate_size: 896
hidden_act: gelu
initializer_range: 0.02
layer_norm_eps: 1.0e-12
model_compilation:
  enable: true
  mode: reduce-overhead
  backend: inductor
  dynamic: false
total_batch_size: 512
per_device_train_batch_size: 64
gradient_accumulation_steps: null
batch_size_rampup_steps: 2000
max_length: null
add_bos_token: false
global_shuffling: false
local_shuffling: false
tokenizer: null
model_name_or_path: vit-mor-custom
model_config: null
attn_implementation: flash_attention_2
use_pretrained_weights: false
recursive:
  enable: true
  base_depth: null
  num_recursion: 3
  sharing: middle_cycle
  ln_share: true
  initialization: stepwise
kv_sharing:
  enable: false
  base_depth: null
  num_recursion: null
  sharing: null
  update_cache: false
relaxation:
  enable: false
  skip_first_loop: false
  method: lora
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules:
    - q_proj
    - v_proj
    rank_pattern: null
    alpha_pattern: 2.0
    svd_init: false
  prompt:
    len: 16
mor:
  enable: true
  type: expert
  capacity: 0.8,0.15,0.05
  rand_router: false
  router_type: mlp
  z_loss: true
  z_coeff: 0.0005
  temp: 2.0
  expert:
    cap_warmup_step: 2000
    router_func: sigmoid
    alpha: 0.05
    sampling: aux_loss
    include_first: true
    coeff: 0.01
    gating: weighted
  token:
    bal_warmup_step: 0
    router_func: softmax
    alpha: 1.0
    balancing: loss
    coeff: 0.1
    u: 0.001
    gating: weighted
lr_scheduler_type: warmup_stable_decay
lr_scheduler_kwargs:
  num_decay_steps: 15000
  decay_type: cosine
learning_rate: 0.0001
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.01
precision: bf16
max_grad_norm: 0.5
num_train_steps: 100000
stop_steps: 100000
num_warmup_steps: 2500
save_steps: 2000
logging_steps: 50
save_interval: 0.1
fixed_save_steps: null
save_total_limit: 5
dataloader_num_workers: 8
gradient_checkpointing: true
deepspeed: ds_configs/stage2.config
save_safetensors: true
evaluation:
  enable: false
  eval_steps: 1000
  batch_size: 32
  tasks: null
  device: null
  num_fewshot: null
training_stability:
  loss_scaling: 128
  loss_scale_window: 100
  min_loss_scale: 1
data_augmentation:
  enable: true
  mixup_alpha: 0.2
  cutmix_alpha: 1.0
  rand_augment: true
  auto_augment: false
regularization:
  dropout: 0.1
  stochastic_depth: 0.1
  label_smoothing: 0.1
