{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d531b611",
   "metadata": {},
   "source": [
    "### Expert-Choice MoR Model Analysis\n",
    "\n",
    "This notebook analyzes the number of recursions required per token in expert-choice Mixture-of-Recursion (MoR) models.    \n",
    "This visualizes the number of recursion steps that each subword token undergoes to predict the *next token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed84ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/anaconda3/envs/mor/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from pathlib import Path\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "os.chdir(PROJECT_DIR)\n",
    "from paths import HF_CACHE_DIR; os.environ[\"HF_HOME\"] = HF_CACHE_DIR\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from copy import deepcopy\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "\n",
    "from model.util import load_model_from_config\n",
    "from model.sharing_strategy import SHARING_STRATEGY\n",
    "from model.kv_caches.cache_utils import Cache, DynamicCache, RecursiveDynamicCache\n",
    "from model.mor_model.modeling_llama import MoRLlamaForCausalLM, MoRLlamaModel, MoRBaseModelOutputWithPast, MoRBaseModelOutputWithPast\n",
    "from model.mor_model.expert_choice_router import MoRLlamaDecoderLayer\n",
    "from model.mor_model.util import MoRLayerOutputWithPast\n",
    "from lm_dataset.load_dataset import load_dataset_from_config\n",
    "from util.config import preprocess_config\n",
    "from util.misc import get_torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c93e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_idx = 0 # Index of the sample to be processed\n",
    "\n",
    "# exp_name = \"250720_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\"\n",
    "exp_name = \"250720_pretrain_smollm-360m_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\"\n",
    "# exp_name = \"250720_pretrain_smollm-360m_rec4_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\"\n",
    "# exp_name = \"250720_pretrain_smollm-360m_kv-share_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\"\n",
    "\n",
    "match = re.search(r\"rec(\\d+)\", exp_name)\n",
    "if match:\n",
    "    num_recursion = int(match.group(1))\n",
    "else:\n",
    "    num_recursion = 3\n",
    "if \"expert\" not in exp_name:\n",
    "    raise ValueError(\"This script is for expert choice routing only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bf5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoRMetricLlamaDecoderLayer(MoRLlamaDecoderLayer):\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        prev_selected_tokens: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs]\n",
    "    ):          \n",
    "        total_x = x # clone()\n",
    "        bs, seq_len, hidden_dim = total_x.shape\n",
    "        \n",
    "        if self.training:\n",
    "            self.training_step += 1\n",
    "            if self.cap_warmup_step > 0:\n",
    "                step_ratio = min(1.0, self.training_step / self.cap_warmup_step)\n",
    "                decay_factor = 0.5 * (1.0 + math.cos(math.pi * step_ratio))\n",
    "            else:\n",
    "                decay_factor = 0.0\n",
    "            capacity_factor = self.capacity_factor + (1.0 - self.capacity_factor) * decay_factor\n",
    "        else:\n",
    "            capacity_factor = self.capacity_factor\n",
    "    \n",
    "        # self.top_k = int(self.capacity_factor * config.max_position_embeddings) # max value\n",
    "        # top_k = min(self.top_k, int(capacity_factor * seq_len))\n",
    "        top_k = int(capacity_factor * seq_len)\n",
    "        \n",
    "        # gather the tokens that were processed in the previous layer\n",
    "        if prev_selected_tokens is not None:\n",
    "            x = torch.gather(x, 1, index=prev_selected_tokens.expand(-1, -1, hidden_dim))\n",
    "        \n",
    "        \"\"\"STEP 1: get logits and top_k tokens\"\"\"\n",
    "        if not self.cfg.mor.rand_router:\n",
    "            _router_weights = self.mor_router(x / self.cfg.mor.temp) # [bs, seq_len, 1]\n",
    "        \n",
    "            if self.router_func is None:\n",
    "                router_probs = router_weights = _router_weights\n",
    "            elif self.router_func == \"sigmoid\":\n",
    "                router_weights = F.sigmoid(_router_weights)\n",
    "                router_probs = router_weights * self.cfg.mor.expert.alpha\n",
    "            elif self.router_func == \"tanh\":\n",
    "                router_weights = F.tanh(_router_weights)\n",
    "                router_probs = router_weights * self.cfg.mor.expert.alpha\n",
    "            else:\n",
    "                raise NotImplementedError(\"Router function is not implemented\")\n",
    "            \n",
    "        else:\n",
    "            router_weights = _router_weights = torch.rand(bs, x.shape[1], 1, device=x.device, dtype=x.dtype)\n",
    "            router_probs = router_weights * self.cfg.mor.expert.get(\"alpha\", 0.1)\n",
    "            \n",
    "        weights, selected_tokens = torch.topk(router_probs, top_k, dim=1, sorted=False) # [bs, k, 1]\n",
    "        # IMPORTANT: need to sort indices to keep causal order for those tokens that are processed in a block\n",
    "        selected_tokens, index = torch.sort(selected_tokens, dim=1)\n",
    "        weights = torch.gather(weights, dim=1, index=index)\n",
    "        \n",
    "        \"\"\"STEP 2: expand indices to process batches with _reduced_ seqlen\"\"\"\n",
    "        # We need to expand indices' dimensions from\n",
    "        # [bs, k, 1] to [bs, k, hidden_size] for gathering\n",
    "        indices_expanded = selected_tokens.expand(-1, -1, hidden_dim)\n",
    "        top_k_tokens = torch.gather(x, dim=1, index=indices_expanded)\n",
    "        \n",
    "        sampling_loss = None\n",
    "        sampling_acc = None\n",
    "        topk_acc = None\n",
    "        uniformity = None\n",
    "        dead_token_seq = None\n",
    "    \n",
    "        targets = torch.zeros_like(router_probs, dtype=router_probs.dtype)\n",
    "        src = torch.ones_like(selected_tokens, dtype=targets.dtype)\n",
    "        targets.scatter_(1, selected_tokens, src)\n",
    "        \n",
    "        if self.sampling == \"aux_router\":\n",
    "            logits = self.mlp_router(x.clone().detach())\n",
    "            sampling_loss = self.bce_loss(logits.view(-1), targets.view(-1)) / (bs * logits.shape[1])\n",
    "            prediction = (F.sigmoid(logits) >= 0.5)\n",
    "            correct_predictions = (prediction == targets).view(-1)\n",
    "            sampling_acc = correct_predictions.sum() / (bs * logits.shape[1])\n",
    "            \n",
    "            aux_router_topk = torch.topk(logits, top_k, dim=1, sorted=False)[1]\n",
    "            topk_acc = torch.tensor(0.0, device=logits.device)\n",
    "            for b in range(bs):\n",
    "                topk_acc += torch.isin(selected_tokens[b].view(-1), aux_router_topk[b].view(-1)).sum()\n",
    "            topk_acc = topk_acc / (bs * top_k)\n",
    "            \n",
    "        elif self.sampling == \"aux_loss\":\n",
    "            if self.router_func is None or self.router_func == \"sigmoid\":\n",
    "                sampling_loss = self.bce_loss(_router_weights.view(-1), targets.view(-1)) / (bs * router_weights.shape[1])\n",
    "                prediction = (router_weights >= 0.5)\n",
    "            elif self.router_func == \"tanh\":\n",
    "                sampling_loss = self.bce_loss(_router_weights.view(-1), targets.view(-1)) / (bs * router_weights.shape[1])\n",
    "                prediction = (router_weights >= 0.)\n",
    "            correct_predictions = (prediction == targets).view(-1)\n",
    "            sampling_acc = correct_predictions.sum() / (bs * router_weights.shape[1])\n",
    "            topk_acc = None\n",
    "            \n",
    "        \"\"\"STEP 3: based on total seqlen, prepare input for block forward\"\"\"\n",
    "        # recompute selected_tokens based on total tokens        \n",
    "        if prev_selected_tokens is not None:\n",
    "            selected_tokens = torch.gather(prev_selected_tokens, dim=1, index=selected_tokens)\n",
    "            indices_expanded = selected_tokens.expand(-1, -1, hidden_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _targets = torch.zeros((bs, seq_len, 1), dtype=selected_tokens.dtype, device=selected_tokens.device)\n",
    "            _targets.scatter_(1, selected_tokens, torch.ones_like(selected_tokens))\n",
    "            dead_token_seq = _targets.squeeze(-1).sum(dim=0).squeeze(0)\n",
    "        \n",
    "        \"\"\"STEP 4: forward block\"\"\"     \n",
    "        if 'kv_sharing' in self.cfg and self.cfg.kv_sharing.enable:\n",
    "            top_k_tokens = total_x.clone()\n",
    "                \n",
    "            for blk in self.block:\n",
    "                outputs = blk(\n",
    "                    top_k_tokens,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                    **kwargs\n",
    "                )\n",
    "                top_k_tokens = outputs[0]\n",
    "            top_k_tokens_processed = torch.gather(outputs[0], dim=1, index=indices_expanded)\n",
    "            \n",
    "        else:\n",
    "            if attention_mask is not None: \n",
    "                if attention_mask.dim() == 4: # attn_implementation == \"eager\"\n",
    "                    row_indices = selected_tokens.unsqueeze(1).expand(bs, 1, top_k, attention_mask.shape[-1])  \n",
    "                    mask_rows_selected = torch.gather(attention_mask, 2, row_indices)\n",
    "                    col_indices = selected_tokens.unsqueeze(1).transpose(2, 3).expand(bs, 1, top_k, top_k)\n",
    "                    attention_mask = torch.gather(mask_rows_selected, 3, col_indices)\n",
    "                elif attention_mask.dim() == 2: # TODO\n",
    "                    raise NotImplementedError(\"Attention mask is not implemented for inference phase of MoR\")\n",
    "                else: \n",
    "                    raise NotImplementedError(\"Attention mask has unexpected dimensions\")\n",
    "            \n",
    "            if position_ids is not None: \n",
    "                position_ids = position_ids[:, :top_k]            \n",
    "            if position_embeddings is not None:\n",
    "                head_dim = position_embeddings[0].shape[-1]\n",
    "                position_embeddings = tuple([torch.gather(emb.expand(bs, -1, -1), dim=1, index=selected_tokens.expand(-1, -1, head_dim)) \n",
    "                                                for emb in position_embeddings])\n",
    "            if cache_position is not None:\n",
    "                cache_position = torch.gather(cache_position.expand(bs, -1), dim=1, index=selected_tokens.squeeze(-1)) \n",
    "                \n",
    "            for blk in self.block:\n",
    "                outputs = blk(\n",
    "                    top_k_tokens,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                    **kwargs\n",
    "                )\n",
    "                top_k_tokens = outputs[0]\n",
    "            top_k_tokens_processed = outputs[0]\n",
    "            \n",
    "        \"\"\"STEP 5: combine results\"\"\"\n",
    "        _src = top_k_tokens_processed * weights if self.cfg.mor.expert.get(\"gating\", \"weighted\") == \"weighted\" else top_k_tokens_processed\n",
    "        total_x = torch.scatter_add(\n",
    "            total_x,\n",
    "            dim=1,\n",
    "            index=indices_expanded,\n",
    "            src=_src,\n",
    "        )\n",
    "        \n",
    "        router_z_loss = torch.logsumexp(_router_weights, dim=-1)\n",
    "        router_z_loss = torch.square(router_z_loss)\n",
    "        router_z_loss = router_z_loss.mean()\n",
    "                \n",
    "        return MoRLayerOutputWithPast(\n",
    "            hidden_state=total_x,\n",
    "            attention_weights=outputs[1:],\n",
    "            selected_tokens=selected_tokens,\n",
    "            sampling_loss=sampling_loss,\n",
    "            sampling_acc=sampling_acc,  \n",
    "            sampling_topk_acc=topk_acc,\n",
    "            uniformity=uniformity,\n",
    "            dead_token_seq=dead_token_seq.detach().cpu(),\n",
    "            balancing_loss=None,\n",
    "            balancing_ratio=None,\n",
    "            router_z_loss=router_z_loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e118a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoRMetricLlamaModel(MoRLlamaModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Union[Tuple, MoRBaseModelOutputWithPast]:\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            if \"kv_sharing\" in self.config and self.config.kv_sharing is not None:\n",
    "                kwargs = self.config.kv_sharing\n",
    "                past_key_values = RecursiveDynamicCache(kwargs[\"base_depth\"], kwargs[\"num_recursion\"], kwargs[\"sharing\"])\n",
    "            else:\n",
    "                past_key_values = DynamicCache()\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        \n",
    "        prev_selected_tokens = None\n",
    "        sampling_loss = torch.tensor(0.0, device=hidden_states.device)\n",
    "        sampling_acc_list = []\n",
    "        sampling_topk_acc_list = []\n",
    "        uniformity = None # torch.tensor(0.0, device=hidden_states.device)\n",
    "        dead_token_seq_list = []\n",
    "        balancing_loss = torch.tensor(0.0, device=hidden_states.device)\n",
    "        balancing_ratio = torch.tensor(0.0, device=hidden_states.device)\n",
    "        router_z_loss = torch.tensor(0.0, device=hidden_states.device)\n",
    "        \n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:            \n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # TODO: support MoRLlamaDecoderLayer\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                    position_embeddings,\n",
    "                )\n",
    "            else:\n",
    "                if hasattr(decoder_layer, \"mor\") and decoder_layer.mor:\n",
    "                    layer_outputs = decoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask=causal_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_values,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                        position_embeddings=position_embeddings,\n",
    "                        prev_selected_tokens=prev_selected_tokens,\n",
    "                        **flash_attn_kwargs,\n",
    "                    )\n",
    "                    if decoder_layer.mor_type == \"expert\":\n",
    "                        prev_selected_tokens = layer_outputs.selected_tokens\n",
    "                        if layer_outputs.dead_token_seq is not None:\n",
    "                            dead_token_seq_list.append(layer_outputs.dead_token_seq)\n",
    "                else:\n",
    "                    layer_outputs = decoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask=causal_mask,\n",
    "                        position_ids=position_ids,\n",
    "                        past_key_value=past_key_values,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                        cache_position=cache_position,\n",
    "                        position_embeddings=position_embeddings,\n",
    "                        **flash_attn_kwargs,\n",
    "                    )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        output = MoRBaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values if use_cache else None,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            sampling_loss=sampling_loss,\n",
    "            sampling_acc=sum(sampling_acc_list)/len(sampling_acc_list) if len(sampling_acc_list) > 0 else torch.tensor(0.0, device=hidden_states.device),\n",
    "            sampling_topk_acc=sum(sampling_topk_acc_list)/len(sampling_topk_acc_list) if len(sampling_topk_acc_list) > 0 else torch.tensor(0.0, device=hidden_states.device),\n",
    "            uniformity=uniformity,\n",
    "            dead_token_seq=dead_token_seq_list,\n",
    "            balancing_loss=balancing_loss,\n",
    "            balancing_ratio=balancing_ratio,\n",
    "            router_z_loss=router_z_loss,\n",
    "        )\n",
    "        return output if return_dict else output.to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ad6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoRMetricLlamaForCausalLM(MoRLlamaForCausalLM):    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = MoRMetricLlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def transform_layer_to_mor_expert(self, cfg):\n",
    "        capacity = [float(cap) for cap in cfg.mor.capacity.split(',')]\n",
    "        # warmup_step for capacity_factor\n",
    "        if \"cap_warmup_step\" in cfg.mor.expert and cfg.mor.expert.cap_warmup_step is not None:\n",
    "            cap_warmup_step = cfg.mor.expert.cap_warmup_step\n",
    "        else:\n",
    "            cap_warmup_step = cfg.num_warmup_steps * cfg.gradient_accumulation_steps\n",
    "        \n",
    "        sharing = cfg.recursive.sharing\n",
    "        num_recursion = cfg.recursive.num_recursion        \n",
    "        num_hidden_layers = len(self.model.layers)\n",
    "        \n",
    "        # Cycle sharing is for early-exiting mechanism\n",
    "        if sharing == \"cycle\":\n",
    "            base_depth = num_hidden_layers // num_recursion\n",
    "            self.model.layers = nn.ModuleList(\n",
    "                [\n",
    "                    MoRMetricLlamaDecoderLayer(self.config, nn.ModuleList([self.model.layers[layer_idx + recur_idx * base_depth] for layer_idx in range(base_depth)]), \n",
    "                                         cfg, capacity[recur_idx], cap_warmup_step,) \n",
    "                    for recur_idx in range(num_recursion)\n",
    "                ]\n",
    "            )\n",
    "        elif sharing == \"middle_cycle\":\n",
    "            base_depth = (num_hidden_layers - 2) // num_recursion\n",
    "            self.model.layers = nn.ModuleList(\n",
    "                [self.model.layers[0]] + \\\n",
    "                [\n",
    "                    MoRMetricLlamaDecoderLayer(self.config, nn.ModuleList([self.model.layers[1 + layer_idx + recur_idx * base_depth] for layer_idx in range(base_depth)]), \n",
    "                                         cfg, capacity[recur_idx], cap_warmup_step,)\n",
    "                    for recur_idx in range(num_recursion)\n",
    "                ]\n",
    "                + [self.model.layers[-1]]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98568c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOR_MODEL_CLS = {\n",
    "    \"smollm\": MoRMetricLlamaForCausalLM,\n",
    "}\n",
    "\n",
    "def load_model_from_config(cfg: DictConfig):\n",
    "    assert \"mor\" in cfg and cfg.mor.enable\n",
    "    model_cls = MOR_MODEL_CLS[cfg.model]\n",
    "        \n",
    "    attn_implementation = cfg.get(\"attn_implementation\", \"flash_attention_2\")\n",
    "    torch_dtype = get_torch_dtype(cfg)\n",
    "    \n",
    "    print(\"Initializing model from scratch...\")\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        cfg.model_name_or_path,\n",
    "        attn_implementation=attn_implementation, \n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    \n",
    "    if cfg.get(\"model_config\") is not None:\n",
    "        print(\"Using custom config for vanilla model...\")\n",
    "        for k, v in cfg.model_config.items():\n",
    "            if not hasattr(config, k):\n",
    "                raise ValueError(f\"Config key {k} not found in model config.\")\n",
    "            print(f\" {k}: {v}\")\n",
    "            setattr(config, k, v)\n",
    "    if cfg.get(\"max_length\") and cfg.max_length != config.max_position_embeddings:\n",
    "        warnings.warn(f\"original max_position_embeddings of {config.max_position_embeddings} is changed to {cfg.max_length}\")\n",
    "        setattr(config, \"max_position_embeddings\", cfg.max_length)\n",
    "    return model_cls._from_config(\n",
    "        config, attn_implementation=attn_implementation, torch_dtype=torch_dtype,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4677e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Preprocess Config -------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically determining batch size based on `total_batch_size`\n",
      "total_batch_size              : 1024 (given)\n",
      "torch.cuda.device_count()     : 1\n",
      "per_device_train_batch_size   : 1 (given)\n",
      "gradient_accumulation_steps   : 1024 (computed)\n",
      "actual total batch size       : 1024\n",
      "Setting wandb_run_name: 250720_pretrain_smollm-360m_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\n",
      "Setting output_dir  : 250720_pretrain_smollm-360m_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001\n",
      "No checkpoint directories found matching /home/sangmin/mixture_of_recursions/results/pretrain/250720_pretrain_smollm-360m_rec3_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001/checkpoint-*\n",
      "Using deepspeed config = /home/sangmin/mixture_of_recursions/ds_configs/stage2.config\n",
      "--------------------------------------------------------------------------------\n",
      "Initializing model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangmin/mixture_of_recursions/util/config.py:95: UserWarning: stop_steps (14161) is not divisible by save_steps (1416)\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoRMetricLlamaForCausalLM(\n",
      "  (model): MoRMetricLlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 960)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "      (1-3): 3 x MoRMetricLlamaDecoderLayer(\n",
      "        (block): ModuleList(\n",
      "          (0-9): 10 x LlamaDecoderLayer(\n",
      "            (self_attn): MoRLlamaAttention(\n",
      "              (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "              (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "              (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "              (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "              (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "              (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (mor_router): LinearRouter(\n",
      "          (router): Linear(in_features=960, out_features=1, bias=False)\n",
      "        )\n",
      "        (bce_loss): BCEWithLogitsLoss()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
      "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
      "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(os.path.join(PROJECT_DIR, \"conf/pretrain\", f\"{exp_name}.yaml\"))\n",
    "cfg.per_device_train_batch_size = 1\n",
    "cfg = preprocess_config(cfg)\n",
    "\n",
    "model = load_model_from_config(cfg)\n",
    "\n",
    "if cfg.recursive.get(\"enable\"):        \n",
    "    # KV cache sharing strategy\n",
    "    model, lora_init_dict = SHARING_STRATEGY[cfg.model](cfg, model)\n",
    "\n",
    "if \"kv_sharing\" in cfg and cfg.kv_sharing.get(\"enable\"):\n",
    "    model.set_kv_sharing_config(cfg)\n",
    "\n",
    "if \"mor\" in cfg and cfg.mor.get(\"enable\"):\n",
    "    if cfg.mor.type == \"expert\":\n",
    "        model.transform_layer_to_mor_expert(cfg)\n",
    "    elif cfg.mor.type == \"token\":\n",
    "        model.transform_layer_to_mor_token(cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MoR type {cfg.mor.type}.\")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee294be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
    "\n",
    "try:\n",
    "    state_dict = torch.load(os.path.join(SAVE_DIR, exp_name, \"pytorch_model.bin\"))\n",
    "except FileNotFoundError:\n",
    "    # safetensors\n",
    "    from safetensors.torch import load_file\n",
    "    state_dict = load_file(os.path.join(SAVE_DIR, exp_name, \"model.safetensors\"))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd734d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = 0 # '<|endoftext|>'\n",
    "\n",
    "cfg.dataset = \"fineweb_test\"\n",
    "dataset = load_dataset_from_config(cfg, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ebb5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "dtype = get_torch_dtype(cfg)\n",
    "routing_list = [None for _ in range(cfg.recursive.num_recursion)]\n",
    "\n",
    "for i, sample in enumerate(dataset):    \n",
    "    if i != sampling_idx: continue\n",
    "    \n",
    "    output = model(\n",
    "        input_ids=sample[\"input_ids\"].unsqueeze(0).to(model.device),\n",
    "        attention_mask=sample[\"attention_mask\"].unsqueeze(0).to(model.device),\n",
    "        labels=sample[\"labels\"].unsqueeze(0).to(model.device),\n",
    "        use_cache=True,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    for j in range(cfg.recursive.num_recursion):\n",
    "        if routing_list[j] is None:\n",
    "            routing_list[j] = output.dead_token_seq[j]\n",
    "        else:\n",
    "            routing_list[j] += output.dead_token_seq[j]\n",
    "        \n",
    "    torch.cuda.empty_cache()    \n",
    "    if i == sampling_idx: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca138765",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_len = 120    # Show only left_len tokens at the start\n",
    "right_len = 60    # Show only right_len tokens at the end\n",
    "\n",
    "data = []\n",
    "for i in range(sample[\"input_ids\"].shape[0]):\n",
    "    if i < left_len:\n",
    "        data.append(tokenizer.decode(sample[\"input_ids\"][i].tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    if i >= sample[\"input_ids\"].shape[0] - right_len:\n",
    "        if i == sample[\"input_ids\"].shape[0] - right_len:\n",
    "            data.append(\"... ...\")\n",
    "        data.append(tokenizer.decode(sample[\"input_ids\"][i].tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa55edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"color: black; white-space: pre-wrap;\"><span style=\"background-color: #f7b0a9;\">People</span><span style=\"background-color: #dabfda;\"> who</span><span style=\"background-color: #dabfda;\"> feel</span><span style=\"background-color: #dabfda;\"> comfortable</span><span style=\"background-color: #dabfda;\"> defending</span><span style=\"background-color: #dabfda;\"> their</span><span style=\"background-color: #dabfda;\"> views</span><span style=\"background-color: #aeddfd;\">—</span><span style=\"background-color: #dabfda;\">def</span><span style=\"background-color: #f7b0a9;\">ensively</span><span style=\"background-color: #f7b0a9;\"> confident</span><span style=\"background-color: #aeddfd;\">—</span><span style=\"background-color: #dabfda;\">may</span><span style=\"background-color: #dabfda;\"> also</span><span style=\"background-color: #f7b0a9;\"> eventually</span><span style=\"background-color: #dabfda;\"> change</span><span style=\"background-color: #aeddfd;\"> those</span><span style=\"background-color: #dabfda;\"> views</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> corresponding</span><span style=\"background-color: #dabfda;\"> behaviors</span><span style=\"background-color: #dabfda;\">.</span><span style=\"background-color: #aeddfd;\"> National</span><span style=\"background-color: #f7b0a9;\"> Election</span><span style=\"background-color: #f7b0a9;\"> Studies</span><span style=\"background-color: #f7b0a9;\"> surveys</span><span style=\"background-color: #dabfda;\"> showed</span><span style=\"background-color: #dabfda;\"> that</span><span style=\"background-color: #aeddfd;\"> defensive</span><span style=\"background-color: #f7b0a9;\"> confidence</span><span style=\"background-color: #aeddfd;\"> predicted</span><span style=\"background-color: #dabfda;\"> def</span><span style=\"background-color: #f7b0a9;\">ection</span><span style=\"background-color: #aeddfd;\"> in</span><span style=\"background-color: #aeddfd;\"> the</span><span style=\"background-color: #dabfda;\"> </span><span style=\"background-color: #aeddfd;\">2</span><span style=\"background-color: #dabfda;\">0</span><span style=\"background-color: #aeddfd;\">0</span><span style=\"background-color: #f7b0a9;\">6</span><span style=\"background-color: #dabfda;\"> U</span><span style=\"background-color: #dabfda;\">.</span><span style=\"background-color: #dabfda;\">S</span><span style=\"background-color: #f7b0a9;\">.</span><span style=\"background-color: #f7b0a9;\"> House</span><span style=\"background-color: #dabfda;\"> elections</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #f7b0a9;\"> above</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #aeddfd;\"> beyond</span><span style=\"background-color: #aeddfd;\"> the</span><span style=\"background-color: #f7b0a9;\"> impact</span><span style=\"background-color: #aeddfd;\"> of</span><span style=\"background-color: #f7b0a9;\"> various</span><span style=\"background-color: #f7b0a9;\"> demographic</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> political</span><span style=\"background-color: #dabfda;\"> variables</span><span style=\"background-color: #dabfda;\">.</span><span style=\"background-color: #dabfda;\"> Moreover</span><br><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #aeddfd;\"> defensive</span><span style=\"background-color: #f7b0a9;\"> confidence</span><span style=\"background-color: #aeddfd;\"> was</span><span style=\"background-color: #f7b0a9;\"> also</span><span style=\"background-color: #dabfda;\"> associated</span><span style=\"background-color: #aeddfd;\"> with</span><span style=\"background-color: #f7b0a9;\"> political</span><span style=\"background-color: #f7b0a9;\"> knowledge</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> attention</span><span style=\"background-color: #f7b0a9;\"> to</span><span style=\"background-color: #f7b0a9;\"> politics</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> government</span><span style=\"background-color: #dabfda;\"> affairs</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #aeddfd;\"> but</span><span style=\"background-color: #f7b0a9;\"> not</span><span style=\"background-color: #f7b0a9;\"> attention</span><span style=\"background-color: #aeddfd;\"> to</span><span style=\"background-color: #aeddfd;\"> the</span><span style=\"background-color: #f7b0a9;\"> news</span><span style=\"background-color: #dabfda;\">.</span><span style=\"background-color: #dabfda;\"> Finally</span><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #f7b0a9;\"> males</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #f7b0a9;\"> more</span><span style=\"background-color: #f7b0a9;\"> educated</span><span style=\"background-color: #f7b0a9;\"> citizens</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #f7b0a9;\"> ethnic</span><span style=\"background-color: #dabfda;\"> minorities</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> older</span><span style=\"background-color: #f7b0a9;\"> respondents</span><span style=\"background-color: #aeddfd;\"> had</span><span style=\"background-color: #aeddfd;\"> higher</span><span style=\"background-color: #f7b0a9;\"> reported</span><span style=\"background-color: #aeddfd;\"> defensive</span><span style=\"background-color: #f7b0a9;\"> confidence</span><span style=\"background-color: #aeddfd;\"> than</span><span style=\"background-color: #aeddfd;\"> did</span><span style=\"background-color: #dabfda;\"> females</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #f7b0a9;\"> less</span><span style=\"background-color: #aeddfd;\"> educated</span><span style=\"background-color: #f7b0a9;\"> citizens</span><span style=\"background-color: #aeddfd;\">,</span><span style=\"background-color: #f7b0a9;\"> European</span><span style=\"background-color: #dabfda;\"> Americans</span><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #aeddfd;\"> and</span><span style=\"background-color: #f7b0a9;\"> younger</span><span style=\"background-color: #dabfda;\"> respondents</span><span style=\"background-color: #dabfda;\">.</span><span style=\"background-color: #dabfda;\"> Def</span><span style=\"background-color: #aeddfd;\">ensive</span><br><span style=\"background-color: #FFFFFF;\">... ...</span><span style=\"background-color: #dabfda;\"> </span><span style=\"background-color: #f7b0a9;\">Researchers</span><span style=\"background-color: #aeddfd;\"> Make</span><span style=\"background-color: #dabfda;\"> Le</span><span style=\"background-color: #f7b0a9;\">ap</span><span style=\"background-color: #aeddfd;\"> in</span><span style=\"background-color: #f7b0a9;\"> Stem</span><span style=\"background-color: #f7b0a9;\"> Cell</span><span style=\"background-color: #f7b0a9;\"> Research</span><span style=\"background-color: #aeddfd;\">;</span><span style=\"background-color: #f7b0a9;\"> Cells</span><span style=\"background-color: #f7b0a9;\"> Can</span><span style=\"background-color: #f7b0a9;\"> Now</span><span style=\"background-color: #aeddfd;\"> be</span><span style=\"background-color: #dabfda;\"> Produ</span><span style=\"background-color: #f7b0a9;\">ced</span><span style=\"background-color: #aeddfd;\"> From</span><span style=\"background-color: #aeddfd;\"> Any</span><span style=\"background-color: #f7b0a9;\"> Tissue</span><span style=\"background-color: #aeddfd;\"> </span><span style=\"background-color: #dabfda;\">July</span><span style=\"background-color: #dabfda;\"> </span><span style=\"background-color: #aeddfd;\">2</span><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #dabfda;\"> </span><span style=\"background-color: #dabfda;\">2</span><span style=\"background-color: #dabfda;\">0</span><span style=\"background-color: #aeddfd;\">0</span><span style=\"background-color: #aeddfd;\">8</span><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #f7b0a9;\"> </span><span style=\"background-color: #aeddfd;\">1</span><span style=\"background-color: #f7b0a9;\">0</span><span style=\"background-color: #dabfda;\">:</span><span style=\"background-color: #aeddfd;\">0</span><span style=\"background-color: #f7b0a9;\">3</span><span style=\"background-color: #dabfda;\"> AM</span><span style=\"background-color: #dabfda;\"> </span><span style=\"background-color: #dabfda;\">N</span><span style=\"background-color: #f7b0a9;\">ail</span><span style=\"background-color: #f7b0a9;\"> Polish</span><span style=\"background-color: #f7b0a9;\"> May</span><span style=\"background-color: #f7b0a9;\"> Soon</span><span style=\"background-color: #aeddfd;\"> be</span><span style=\"background-color: #aeddfd;\"> A</span><span style=\"background-color: #f7b0a9;\">ble</span><span style=\"background-color: #f7b0a9;\"> to</span><span style=\"background-color: #dabfda;\"> Det</span><span style=\"background-color: #aeddfd;\">ect</span><span style=\"background-color: #f7b0a9;\"> Date</span><span style=\"background-color: #dabfda;\"> R</span><span style=\"background-color: #f7b0a9;\">ape</span><span style=\"background-color: #f7b0a9;\"> Drugs</span><span style=\"background-color: #aeddfd;\"> </span><span style=\"background-color: #dabfda;\">August</span><span style=\"background-color: #aeddfd;\"> </span><span style=\"background-color: #aeddfd;\">2</span><span style=\"background-color: #f7b0a9;\">6</span><span style=\"background-color: #dabfda;\">,</span><span style=\"background-color: #dabfda;\"> </span><br></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file saved successfully as 'highlighted_text_multiline.html'.\n",
      "Open this file directly in a web browser to see if it appears on multiple lines.\n"
     ]
    }
   ],
   "source": [
    "HIGHLIGHT_COLORS = {\n",
    "    \"purple\": \"#dabfda\",\n",
    "    \"red\": \"#f7b0a9\",\n",
    "    \"blue\": \"#aeddfd\",\n",
    "    \"yellow\": \"#f2e4a5\",\n",
    "    \"no_highlight\": \"transparent\"\n",
    "}\n",
    "\n",
    "def get_layer_value_safe(dts_list, layer_idx, token_idx, default_val=0):\n",
    "    # This function safely retrieves a value from a nested list.\n",
    "    if layer_idx < len(dts_list):\n",
    "        layer = dts_list[layer_idx]\n",
    "        if hasattr(layer, '__len__') and token_idx < len(layer):\n",
    "            val = layer[token_idx]\n",
    "            if hasattr(val, 'item') and callable(val.item):\n",
    "                return val.item()\n",
    "            elif isinstance(val, (int, float)):\n",
    "                 return val\n",
    "            elif not hasattr(val, '__len__'): # if it looks like a scalar\n",
    "                return val\n",
    "    return default_val\n",
    "\n",
    "def generate_highlighted_text_html(tokenizer, sample_data, current_left_len, current_right_len, current_dead_token_seq_list, current_colors, tokens_per_line=15):\n",
    "    \"\"\"\n",
    "    Analyzes a token sequence, applies background colors based on conditions,\n",
    "    and generates an HTML string with line breaks (<br>) after a specified number of tokens.\n",
    "    \"\"\"\n",
    "    total_tokens = sample_data[\"input_ids\"].shape[0]\n",
    "    token_highlight_colors = [current_colors[\"no_highlight\"]] * total_tokens\n",
    "    recursion_counts = [0] * total_tokens  # Initialize recursion counts for each token\n",
    "\n",
    "    # 1. Determine the highlight color and recursion count for each token in advance\n",
    "    for j in range(total_tokens):\n",
    "        l0_val = get_layer_value_safe(current_dead_token_seq_list, 0, j)\n",
    "        l1_val = get_layer_value_safe(current_dead_token_seq_list, 1, j) if len(current_dead_token_seq_list) > 1 else 0\n",
    "        l2_val = get_layer_value_safe(current_dead_token_seq_list, 2, j) if len(current_dead_token_seq_list) > 2 else 0\n",
    "        l3_val = get_layer_value_safe(current_dead_token_seq_list, 3, j) if len(current_dead_token_seq_list) > 3 else 0\n",
    "\n",
    "        chosen_color_key = None\n",
    "        recursion_count = 0\n",
    "\n",
    "        if l0_val == 1 and l1_val == 1 and l2_val == 1 and l3_val == 1:\n",
    "            chosen_color_key = \"yellow\"\n",
    "            recursion_count = 4\n",
    "        elif l0_val == 1 and l1_val == 1 and l2_val == 1 and l3_val == 0:\n",
    "            chosen_color_key = \"red\"\n",
    "            recursion_count = 3\n",
    "        elif l0_val == 1 and l1_val == 1 and l2_val == 0 and l3_val == 0:\n",
    "            chosen_color_key = \"blue\"\n",
    "            recursion_count = 2\n",
    "        elif l0_val == 1 and l1_val == 0 and l2_val == 0 and l3_val == 0:\n",
    "            chosen_color_key = \"purple\"\n",
    "            recursion_count = 1\n",
    "        \n",
    "        if chosen_color_key:\n",
    "            token_highlight_colors[j] = current_colors[chosen_color_key]\n",
    "        recursion_counts[j] = recursion_count\n",
    "\n",
    "    # 2. Prepare HTML fragment list and line break logic\n",
    "    html_parts = []\n",
    "    token_count_on_current_line = 0\n",
    "\n",
    "    def append_html_and_conditionally_break(html_element, is_actual_token=True):\n",
    "        nonlocal token_count_on_current_line\n",
    "        html_parts.append(html_element)\n",
    "        if is_actual_token:\n",
    "            token_count_on_current_line += 1\n",
    "            if token_count_on_current_line >= tokens_per_line:\n",
    "                html_parts.append(\"<br>\")\n",
    "                token_count_on_current_line = 0\n",
    "\n",
    "    def get_processed_token_string(token_id_tensor_obj):\n",
    "        token_ids_list_for_decode = token_id_tensor_obj.tolist() if hasattr(token_id_tensor_obj, 'tolist') else [token_id_tensor_obj]\n",
    "        token_string = tokenizer.decode(\n",
    "            token_ids_list_for_decode,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        token_string = token_string.replace('\\n', ' ').replace('\\r', '') # Remove internal line breaks\n",
    "        return html.escape(token_string)\n",
    "\n",
    "    # # Process left part tokens\n",
    "    last_idx_of_left_part = -1\n",
    "    for i in range(min(current_left_len, total_tokens)):\n",
    "        escaped_token_string = get_processed_token_string(sample_data[\"input_ids\"][i])\n",
    "        current_token_color = token_highlight_colors[i]\n",
    "        recursion_count = recursion_counts[i]\n",
    "        \n",
    "        part_to_add = \"\"\n",
    "        if current_token_color != current_colors[\"no_highlight\"] and current_token_color is not None:\n",
    "            part_to_add = f'<span style=\"background-color: {current_token_color};\">{escaped_token_string}</span>'\n",
    "        else:\n",
    "            part_to_add = f'{escaped_token_string}'\n",
    "        append_html_and_conditionally_break(part_to_add, is_actual_token=True)\n",
    "        last_idx_of_left_part = i\n",
    "\n",
    "    # Check and insert \"...\" condition\n",
    "    first_idx_of_right_part = total_tokens - current_right_len\n",
    "    if first_idx_of_right_part > last_idx_of_left_part + 1:\n",
    "        # \"...\" is not included in the token count and is processed so that it does not cause a line break.\n",
    "        # Add <br> here if you want a forced line break before/after \"...\"\n",
    "        append_html_and_conditionally_break('<span style=\"background-color: #FFFFFF;\">... ...</span>', is_actual_token=False)\n",
    "\n",
    "\n",
    "    # Process right part tokens\n",
    "    for i in range(max(0, first_idx_of_right_part), total_tokens):\n",
    "        if i > last_idx_of_left_part:\n",
    "            escaped_token_string = get_processed_token_string(sample_data[\"input_ids\"][i])\n",
    "            current_token_color = token_highlight_colors[i]\n",
    "            recursion_count = recursion_counts[i]\n",
    "\n",
    "            part_to_add = \"\"\n",
    "            if current_token_color != current_colors[\"no_highlight\"] and current_token_color is not None:\n",
    "                part_to_add = f'<span style=\"background-color: {current_token_color};\">{escaped_token_string}</span>'\n",
    "            else:\n",
    "                part_to_add = f'{escaped_token_string}'\n",
    "            append_html_and_conditionally_break(part_to_add, is_actual_token=True)\n",
    "            \n",
    "    inner_html = \"\".join(html_parts)\n",
    "    # Change to white-space: pre-wrap; so that the <br> tag works and other spaces are also (partially) maintained.\n",
    "    return f'<div style=\"color: black; white-space: pre-wrap;\">{inner_html}</div>', recursion_counts\n",
    "\n",
    "highlighted_html_content, recursion_counts = generate_highlighted_text_html(\n",
    "    tokenizer,\n",
    "    sample,\n",
    "    left_len,\n",
    "    right_len,\n",
    "    routing_list,\n",
    "    HIGHLIGHT_COLORS,\n",
    "    tokens_per_line=60 # Number of tokens to display per line\n",
    ")\n",
    "\n",
    "# 2. Display HTML in Jupyter Notebook, etc.:\n",
    "\n",
    "display(HTML(highlighted_html_content))\n",
    "\n",
    "# --- Code to save as HTML file ---\n",
    "html_file_to_save = \"highlighted_text_multiline.html\" \n",
    "\n",
    "try:\n",
    "    with open(html_file_to_save, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"<!DOCTYPE html>\\n\")\n",
    "        f.write(\"<html lang=\\\"ko\\\">\\n\")\n",
    "        f.write(\"<head>\\n\")\n",
    "        f.write(\"    <meta charset=\\\"UTF-8\\\">\\n\")\n",
    "        f.write(\"    <title>Highlighted Text Output (Multiline)</title>\\n\")\n",
    "        f.write(\"    <style>\\n\")\n",
    "        f.write(\"        body { \\n\")\n",
    "        f.write(\"            font-family: sans-serif; \\n\")\n",
    "        f.write(\"            line-height: 1.6; \\n\")\n",
    "        f.write(\"            margin: 20px; \\n\")\n",
    "        #  white-space: pre-wrap; is applied to this div, so no special white-space setting is required for the body\n",
    "        f.write(\"        }\\n\")\n",
    "        f.write(\"    </style>\\n\")\n",
    "        f.write(\"</head>\\n\")\n",
    "        f.write(\"<body>\\n\")\n",
    "        f.write(highlighted_html_content) # Write the generated HTML body content (including div)\n",
    "        f.write(\"\\n</body>\\n\")\n",
    "        f.write(\"</html>\\n\")\n",
    "    print(f\"HTML file saved successfully as '{html_file_to_save}'.\")\n",
    "    print(f\"Open this file directly in a web browser to see if it appears on multiple lines.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9f52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
