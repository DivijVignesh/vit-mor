name: imagenet_vit_mor_optimized

wandb: true
wandb_mode: online
wandb_entity: divijvignesh1223
wandb_project: vit-mor-experiments
wandb_run_name: null
output_dir: vit_mor_imagenet
wandb_run_id: 66GYUq8K # run `python util/generate_wandb_run_id.py` to generate a new id

tensorboard: false
tensorboard_dir: null

resume_from_checkpoint: false
resume_step: null

# Vision-specific settings
model: vit_mor
dataset: imagenet1k
image_size: 224
patch_size: 16
num_labels: 1000
hidden_size: 768        # Up from 198
num_hidden_layers: 7    # Up from 6  
num_attention_heads: 7  # Up from 6
intermediate_size: 896 # Up from 768
hidden_act: gelu
initializer_range: 0.02
layer_norm_eps: 1e-12

model_compilation:
  enable: true
  mode: "reduce-overhead"
  backend: "inductor"
  dynamic: false

# OPTIMIZED: Training configuration with better batch size progression
total_batch_size: 512
per_device_train_batch_size: 64    # REDUCED: Start smaller for stability
gradient_accumulation_steps: null    # ADDED: To maintain total batch size
batch_size_rampup_steps: 2000      # ADDED: Gradual batch size increase
max_length: null
add_bos_token: false
global_shuffling: false
local_shuffling: false
tokenizer: null

model_name_or_path: "vit-mor-custom"
model_config: null
attn_implementation: flash_attention_2
use_pretrained_weights: false

# OPTIMIZED: MoR Configuration - Critical Changes
recursive:
  enable: true
  base_depth: null
  num_recursion: 3
  sharing: middle_cycle
  ln_share: true
  initialization: stepwise

kv_sharing:
  enable: false
  base_depth: null
  num_recursion: null
  sharing: null
  update_cache: false

relaxation:
  enable: false
  skip_first_loop: false
  method: lora
  lora:
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: ['q_proj', 'v_proj']
    rank_pattern: null
    alpha_pattern: 2.0
    svd_init: false
  prompt:
    len: 16

# CRITICAL FIXES: MoR routing configuration
mor:
  enable: true
  type: expert
  capacity: "0.8,0.15,0.05"          # CHANGED: More balanced capacity distribution
  rand_router: false
  router_type: mlp                 # CHANGED: More expressive router
  z_loss: true
  z_coeff: 0.0008                   # INCREASED: From 0.00001 to 0.001 for better load balancing
  temp: 2.0                        # INCREASED: From 1.0 to 2.0 for smoother routing
  expert:
    cap_warmup_step: 2000          # INCREASED: Longer warmup for stability
    router_func: sigmoid
    alpha: 0.05                    # REDUCED: From 0.1 to 0.05 for gentler routing
    sampling: aux_loss
    include_first: true 
    coeff: 0.01                    # INCREASED: From 0.001 to 0.01 for better sampling
    gating: weighted
  token:
    bal_warmup_step: 0
    router_func: softmax
    alpha: 1.0
    balancing: loss
    coeff: 0.1
    u: 0.001
    gating: weighted

# OPTIMIZED: Training hyperparameters
lr_scheduler_type: warmup_stable_decay
lr_scheduler_kwargs:
  num_decay_steps: 15000           # ADJUSTED: Better decay schedule
  decay_type: cosine
learning_rate: 1e-4                # REDUCED: From 3e-4 to 1e-4 for stability
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.01                 # REDUCED: From 0.05 to 0.01, less aggressive
precision: bf16
max_grad_norm: 0.5                 # REDUCED: From 1.0 to 0.5 for better stability

# OPTIMIZED: Training schedule
num_train_steps: 100000             # INCREASED: More steps for better convergence
stop_steps: 100000
num_warmup_steps: 2500             # INCREASED: Longer warmup (14% instead of 10%)
save_steps: 2000                   # REDUCED: More frequent saves
logging_steps: 50                  # REDUCED: More frequent logging
save_interval: 0.1
fixed_save_steps: null
save_total_limit: 5                # INCREASED: Keep more checkpoints
dataloader_num_workers: 8          # REDUCED: Less aggressive data loading
gradient_checkpointing: true       # ENABLED: Memory efficiency
deepspeed: ds_configs/stage2.config
save_safetensors: true

# ENABLED: Evaluation for monitoring
evaluation:
  enable: false                     # ENABLED: Monitor validation performance
  eval_steps: 1000                 # Regular evaluation
  batch_size: 32                   # Conservative eval batch size
  tasks: null
  device: null
  num_fewshot: null

# ADDED: Additional stability configurations
training_stability:
  loss_scaling: 128                # For mixed precision stability
  loss_scale_window: 100           # Loss scaling window
  min_loss_scale: 1                # Minimum loss scale
  
# ADDED: Data augmentation (if supported)
data_augmentation:
  enable: true
  mixup_alpha: 0.2                 # MixUp augmentation
  cutmix_alpha: 1.0                # CutMix augmentation
  rand_augment: true               # RandAugment
  auto_augment: false              # Disable AutoAugment initially

# ADDED: Model regularization
regularization:
  dropout: 0.1                     # Add dropout for regularization
  stochastic_depth: 0.1            # Stochastic depth for ViT
  label_smoothing: 0.1             # Label smoothing for better generalization